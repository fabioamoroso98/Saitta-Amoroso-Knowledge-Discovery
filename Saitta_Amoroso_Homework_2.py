# -*- coding: utf-8 -*-
"""Saitta-Amoroso-Homework-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ISaan7d0OU4mupjcyWXxxhZLPJ0-8Map
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import urllib
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
import seaborn as sn
from matplotlib import pyplot as plt

# install kaggle
! pip install -q kaggle

from google.colab import files

files.upload()

# create a kaggle folder
! mkdir ~/.kaggle

# copy the kaggle.json to folder created
! cp kaggle.json ~/.kaggle/

#permission for the json to act
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download hamditarek/imdb-dataset-50k-maas-et-al-2011

! kaggle datasets download kritanjalijain/amazon-reviews

! kaggle datasets download ilhamfp31/yelp-review-dataset

! unzip imdb-dataset-50k-maas-et-al-2011.zip

! unzip amazon-reviews.zip

! unzip yelp-review-dataset.zip

amazon = pd.read_csv("test.csv", names= ["label", "review"], usecols = [0,2])
imdb = pd.read_csv("IMDB_Movies.csv",header = 0, names = ["review", "label"])
yelp = pd.read_csv("yelp_review_polarity_csv/train.csv", names= ["label", "review"])

#data preprocessing to make uniform the three parts
imdb = imdb[imdb.columns[::-1]]
amazon["label"] = amazon["label"].map({2:1, 1:0})
yelp["label"] = yelp["label"].map({2:1, 1:0})

d = pd.DataFrame()
df = [amazon, imdb, yelp]
for data in df:
  positive = data.loc[(data["label"] == 1)]
  negative = data.loc[(data["label"] == 0)]
  positive = positive[:5000]
  negative = negative[:5000]
  d = d.append(positive)
  d = d.append(negative)
labels = d.iloc[:, 0]
reviews = d.iloc[:, 1]

np.savetxt(r'reviews.txt', reviews, fmt='%s')
np.savetxt(r'labels.txt', labels, fmt='%s')

# Load reviews to string
reviews_path = "reviews.txt"
with open(reviews_path, "r") as f:
    reviews = f.read()

len(reviews)

# Converting to Lowercase
reviews = reviews.lower()

# print the first 2000 characters of this string
reviews[:2000]

"""Now, we have to remove all puntuaction simbols inside the sentences. """

# Remove punctuaction
from string import punctuation

list_characters = [c for c in reviews if c not in punctuation]

reviews = "".join(list_characters)

reviews[:1000]

# split the reviews
reviews = reviews.split("\n")

reviews[:10]

len(reviews)

#last elem in reviews list is an empty string, so discard it
print(reviews[30000])
reviews = reviews[:-1]

"""now `reviews` is a list of sentences:"""

# each element of reviews is a review
reviews[:10]

"""Since the input of our model will be the individual word, now we have to split the words inside the sentence"""

reviews = [[w for w in r.split(" ") if len(w)>0] for r in reviews]

"""now `reviews` is a list of list. Each reaview is a list of words."""

# compute the length of each review
reviews_lens = [len(r) for r in reviews]
# print the lenght of the first x reviews
reviews_lens[:10]

# print the average review lenght
sum(reviews_lens) / len(reviews_lens)

max(reviews_lens)

"""Our rnn cannot use words directly, we have to convert each word in a number. When we pass a review to the model, the model will see a sequence of numbers.
To do this, let's create the vocabulary of our words.
"""

# build vocabulary
words = list(set([w for r in reviews for w in r]))
vocab = {words[i]: i+1 for i in range(len(words))} # we reserve i=0 for pad sequences

len(vocab)

# Convert reviews to word indexes
reviews = [[vocab[w] for w in r] for r in reviews]

"""The reviews have different lengths, while, we want that all reviews have the same length

"""

seq_len = 200    # number of words for each sentence
#we do it because of the bach structure not because CNN cannot veal with different dimensions.

# Clip reviews to max seq_len words
#resize the reviews with same len = seq_len
reviews = [r[:seq_len] for r in reviews]

# Print average review length now
review_lens = [len(r) for r in reviews]
print(sum(review_lens)/len(review_lens))

# Pad reviews shorter than seq_len
# TODO test padding at the end
reviews = [[0]*(seq_len - len(r)) + r for r in reviews]

# Print average review length now
review_lens = [len(r) for r in reviews]
print(sum(review_lens)/len(review_lens))

"""Now, `reviews` is a list of list; each elem of `reviews` is a list with `seq_len` elements. So we can convert `reviews` in a Tensor:

"""

# Convert reviews to tensor
data = torch.LongTensor(reviews)

"""Finally the data tensor is ready. Now it's the turn of the labels.

Since we are solving two tasks, we create two distinct sets of labels
"""

# Load labels
labels_path = "labels.txt"
with open(labels_path, "r") as f:
    labels = f.read()

labels = labels.split("\n")

labels = labels[:-1]

# to convert list of string to list of int
labels = [int(x) for x in labels]

# Convert sentiments to tensor
labels = torch.LongTensor(labels)

# create topics' labels
amazon_topic = np.zeros(10000)
imdb_topic = np.ones(10000)
yelp_topic = np.ones(10000) * 2

amazon_topic = torch.LongTensor(amazon_topic)
imdb_topic = torch.LongTensor(imdb_topic)
yelp_topic = torch.LongTensor(yelp_topic)

labels_topics = torch.cat((amazon_topic, imdb_topic, yelp_topic), 0)

"""### Defining the Datasets"""

# some parameters
frac_train = 0.8 # fraction of data for train
frac_val = 0.1   # fraction of data for val
batch_size = 64

# shuffle dataset
num_data = data.size(0)
shuffle_idx = torch.randperm(num_data)

data = data[shuffle_idx,:]
labels = labels[shuffle_idx]
labels_topics = labels_topics[shuffle_idx]

# split training, validation and test
num_train = int(num_data*frac_train)
num_val = int(num_data*frac_val)
num_test = num_data - num_train - num_val

train_data = data[:num_train,:]
train_labels = labels[:num_train]
train_labels_topics = labels_topics[:num_train]

val_data = data[num_train:num_train+num_val,:]
val_labels = labels[num_train:num_train+num_val]
val_labels_topics = labels_topics[num_train:num_train+num_val]

test_data = data[num_train+num_val:,:]
test_labels = labels[num_train+num_val:]
test_labels_topics = labels_topics[num_train+num_val:]

# create datasets
train_dataset = TensorDataset(train_data, train_labels)
val_dataset = TensorDataset(val_data, val_labels)
test_dataset = TensorDataset(test_data, test_labels)

train_dataset_topics = TensorDataset(train_data, train_labels_topics)
val_dataset_topics = TensorDataset(val_data, val_labels_topics)
test_dataset_topics = TensorDataset(test_data, test_labels_topics)

# Create loaders
loaders = {"train": DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  drop_last=True),
           "val":   DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, drop_last=False),
           "test":  DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, drop_last=False)}

loaders_topics = {"train": DataLoader(train_dataset_topics, batch_size=batch_size, shuffle=True,  drop_last=True),
                  "val":   DataLoader(val_dataset_topics,   batch_size=batch_size, shuffle=False, drop_last=False),
                  "test":  DataLoader(test_dataset_topics,  batch_size=batch_size, shuffle=False, drop_last=False)}

"""## Defining our RNNs

The [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) module:



Basically, the Embedding module translates numbers in vectors. The *translation function* is learned by this module during the training.
"""

# Define model
class Model_topics(nn.Module):
    
    def __init__(self, num_embed, embed_size, rnn_size):
        # params: 
        # num_embed: the number of the input vocabulary
        # embed_size: the size of the feature embedding
        # rnn_size: the number of neurons in the recurrent layer

        # Call parent constructor
        super().__init__()
        # Store values
        self.rnn_size = rnn_size
        # Define modules
        self.embedding = nn.Embedding(len(vocab)+1, embed_size)
        self.rnn = nn.LSTMCell(embed_size, rnn_size) #LSTMCell represents only a single time step
        self.output = nn.Linear(rnn_size, 3)

    def forward(self, x):
        # Embed data
        x = self.embedding(x)
        # Initialize state
        h = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the state of the cell
        c = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the state of the cell

        # Input is: B x T x F
        # Process each time step
        for t in range(x.shape[1]):
            # Input at time t
            x_t = x[:,t,:]
            # Forward RNN and get new state
            h, c = self.rnn(x_t, (h, c))
        # Classify final state
        x = self.output(h)
        return x

# Setup device
dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model parameters
embed_size = 256
rnn_size = 64

# Create model
model = Model_topics(len(vocab)+1, embed_size, rnn_size).to(dev)

# Test model output
model.eval()
test_input = train_dataset[0][0].unsqueeze(0).to(dev)
print("Model output size:", model(test_input).size())

# Create optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=2e-4)

# Define a loss 
criterion = nn.CrossEntropyLoss()

# Start training
from tqdm.notebook import tqdm

best_test_accuracy = 0

for epoch in range(35):
    # Initialize accumulators for computing average loss/accuracy
    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}
    # Process each split
    for split in ["train", "val", "test"]:
        # Set network mode
        if split == "train":
            model.train()
            torch.set_grad_enabled(True)
        else:
            model.eval()
            torch.set_grad_enabled(False)
        # Process all data in split
        for input, target in tqdm(loaders_topics[split]):
            # Move to device
            input = input.to(dev)
            target = target.to(dev)
            # Reset gradients
            optimizer.zero_grad()
            # Forward
            output = model(input)
            loss = criterion(output, target)
            # Update loss sum
            epoch_loss_sum[split] += loss.item()
            epoch_loss_cnt[split] += 1
            # Compute accuracy
            _,pred = output.max(1)
            correct = pred.eq(target).sum().item()
            accuracy = correct/input.size(0)
            # Update accuracy sum
            epoch_accuracy_sum[split] += accuracy
            epoch_accuracy_cnt[split] += 1
            # Backward and optimize
            if split == "train":
                loss.backward()
                optimizer.step()
    # Compute average epoch loss/accuracy
    avg_train_loss = epoch_loss_sum["train"]/epoch_loss_cnt["train"]
    avg_train_accuracy = epoch_accuracy_sum["train"]/epoch_accuracy_cnt["train"]
    avg_val_loss = epoch_loss_sum["val"]/epoch_loss_cnt["val"]
    avg_val_accuracy = epoch_accuracy_sum["val"]/epoch_accuracy_cnt["val"]
    avg_test_loss = epoch_loss_sum["test"]/epoch_loss_cnt["test"]
    avg_test_accuracy = epoch_accuracy_sum["test"]/epoch_accuracy_cnt["test"]
    print(f"Epoch: {epoch+1}, TrL={avg_train_loss:.4f}, TrA={avg_train_accuracy:.4f},",
                            f"VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ",
                            f"TeL={avg_test_loss:.4f}, TeA={avg_test_accuracy:.4f}")
    # Store params at the best test accuracy
    if avg_test_accuracy > best_test_accuracy:
       torch.save(model.state_dict(), f"model_best_test_topics.pth")
       best_test_accuracy = avg_test_accuracy

y_pred = []
y_true = []

model.load_state_dict(torch.load("model_best_test_topics.pth"))
model.eval()

# iterate over test data
for inputs, labels in loaders_topics["test"]:
        inputs = inputs.cuda()
        labels = labels.cuda()
        output = model(inputs) # Feed Network

        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()
        y_pred.extend(output) # Save Prediction
        
        labels = labels.data.cpu().numpy()
        y_true.extend(labels) # Save Truth

# constant for classes
classes = ['Amazon', 'IMDB', 'Yelp']

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, display_labels=classes)
disp.plot()
plt.show()

# Define model
class Model(nn.Module):
    
    def __init__(self, num_embed, embed_size, rnn_size):
        # params: 
        # num_embed: the number of the input vocabulary
        # embed_size: the size of the feature embedding
        # rnn_size: the number of neurons in the recurrent layer

        # Call parent constructor
        super().__init__()
        # Store values
        self.rnn_size = rnn_size
        # Define modules
        self.embedding = nn.Embedding(len(vocab)+1, embed_size)
        self.rnn = nn.LSTMCell(embed_size, rnn_size) #LSTMCell represents only a single time step
        self.output = nn.Linear(rnn_size, 2)

    def forward(self, x):
        # Embed data
        x = self.embedding(x)
        # Initialize state
        h = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the state of the cell
        c = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the state of the cell

        # Input is: B x T x F
        # Process each time step
        for t in range(x.shape[1]):
            # Input at time t
            x_t = x[:,t,:]
            # Forward RNN and get new state
            h, c = self.rnn(x_t, (h, c))
        # Classify final state
        x = self.output(h)
        return x

# Model parameters
embed_size = 128
rnn_size = 32

# Create model
model = Model(len(vocab)+1, embed_size, rnn_size).to(dev)

# Test model output
model.eval()
test_input = train_dataset[0][0].unsqueeze(0).to(dev)
print("Model output size:", model(test_input).size())

# Create optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.008, weight_decay=5e-4)

# Define a loss 
criterion = nn.CrossEntropyLoss()

# Start training
from tqdm.notebook import tqdm

best_test_accuracy = 0

for epoch in range(35):
    # Initialize accumulators for computing average loss/accuracy
    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}
    # Process each split
    for split in ["train", "val", "test"]:
        # Set network mode
        if split == "train":
            model.train()
            torch.set_grad_enabled(True)
        else:
            model.eval()
            torch.set_grad_enabled(False)
        # Process all data in split
        for input, target in tqdm(loaders[split]):
            # Move to device
            input = input.to(dev)
            target = target.to(dev)
            # Reset gradients
            optimizer.zero_grad()
            # Forward
            output = model(input)
            loss = criterion(output, target)
            # Update loss sum
            epoch_loss_sum[split] += loss.item()
            epoch_loss_cnt[split] += 1
            # Compute accuracy
            _,pred = output.max(1)
            correct = pred.eq(target).sum().item()
            accuracy = correct/input.size(0)
            # Update accuracy sum
            epoch_accuracy_sum[split] += accuracy
            epoch_accuracy_cnt[split] += 1
            # Backward and optimize
            if split == "train":
                loss.backward()
                optimizer.step()
    # Compute average epoch loss/accuracy
    avg_train_loss = epoch_loss_sum["train"]/epoch_loss_cnt["train"]
    avg_train_accuracy = epoch_accuracy_sum["train"]/epoch_accuracy_cnt["train"]
    avg_val_loss = epoch_loss_sum["val"]/epoch_loss_cnt["val"]
    avg_val_accuracy = epoch_accuracy_sum["val"]/epoch_accuracy_cnt["val"]
    avg_test_loss = epoch_loss_sum["test"]/epoch_loss_cnt["test"]
    avg_test_accuracy = epoch_accuracy_sum["test"]/epoch_accuracy_cnt["test"]
    print(f"Epoch: {epoch+1}, TrL={avg_train_loss:.4f}, TrA={avg_train_accuracy:.4f},",
                            f"VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ",
                            f"TeL={avg_test_loss:.4f}, TeA={avg_test_accuracy:.4f}")
    # Store params at the best test accuracy
    if avg_test_accuracy > best_test_accuracy:
      torch.save(model.state_dict(), f"model_best_test.pth")
      best_test_accuracy = avg_test_accuracy

y_pred = []
y_true = []

model.load_state_dict(torch.load("model_best_test.pth"))
model.eval()

# iterate over test data
for inputs, labels in loaders["test"]:
        inputs = inputs.cuda()
        labels = labels.cuda()
        output = model(inputs) # Feed Network

        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()
        y_pred.extend(output) # Save Prediction
        
        labels = labels.data.cpu().numpy()
        y_true.extend(labels) # Save Truth

# constant for classes
classes = ['Negative', 'Positive']

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, display_labels=classes)
disp.plot()
plt.show()